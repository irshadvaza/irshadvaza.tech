# ğŸš€ Chapter 2: Creating Azure Data Factory (Step-by-Step Guide)

---

# ğŸ¯ Objective of This Chapter

In this chapter, you will learn:

- How to create Azure Data Factory
- How to navigate ADF Studio
- How to understand the UI layout
- How to configure Git integration
- How to publish your first workspace

By the end of this chapter, you will have your own working Azure Data Factory.

---

# ğŸ—ï¸ Step 1: Login to Azure Portal

1. Open browser
2. Go to: https://portal.azure.com
3. Login with your Azure account

You will land on the **Azure Portal Dashboard**.

---

# ğŸ” Step 2: Search for Azure Data Factory

In the top search bar:

Type:

```
Data Factory
```

Click on:

**Data Factories**

Then click:

â• **Create**

---

# âš™ï¸ Step 3: Configure Basic Settings

You will now see the "Create Data Factory" page.

Fill the following details:

## ğŸ”¹ Subscription
Select your Azure subscription.

## ğŸ”¹ Resource Group
Choose:
- Existing resource group  
OR  
- Create new (Recommended for learning)

Example:
```
rg-adf-learning
```

## ğŸ”¹ Name
Enter a globally unique name:

Example:
```
adf-learning-irshad
```

## ğŸ”¹ Region
Choose nearest region (Example: UAE North)

## ğŸ”¹ Version
Select:
```
V2 (Latest)
```

Click:
```
Review + Create
```

Then:
```
Create
```

Deployment takes 1â€“2 minutes.

---

# âœ… Step 4: Go to Resource

Once deployment completes:

Click:
```
Go to Resource
```

You will now see your Azure Data Factory Overview page.

---

# ğŸ¨ Step 5: Open ADF Studio

Click:

```
Launch Studio
```

This opens the **Azure Data Factory Studio UI**

This is where you build pipelines.

---

# ğŸ–¥ï¸ Understanding Azure Data Factory Studio (UI Walkthrough)

When Studio opens, you will see a clean UI with left-side navigation.

---

# ğŸ“‚ ADF Studio Layout Overview

Left Panel Icons:

| Icon | Section | Purpose |
|------|---------|----------|
| ğŸ  | Home | Quick start templates |
| âœï¸ | Author | Create pipelines & datasets |
| ğŸ” | Monitor | View pipeline runs |
| ğŸ§° | Manage | Linked services & IR |
| ğŸ“˜ | Learn | Documentation |

---

# ğŸ§­ Step 6: Explore Author Section

Click:

âœï¸ **Author**

Here you will create:

- Pipelines
- Datasets
- Dataflows

Right now, it is empty because we haven't created anything yet.

---

# ğŸ§° Step 7: Configure Linked Service (First Connection)

Before building pipelines, we must create a connection.

Click:

ğŸ§° **Manage**

Then:

â• **New Linked Service**

You will see 100+ connectors like:

- Azure SQL Database
- Blob Storage
- REST API
- SQL Server
- Databricks

For learning, select:

**Azure Blob Storage**

Click:
```
Continue
```

---

# ğŸ” Configure Blob Storage Connection

Fill:

- Name: `ls_blob_storage`
- Authentication: Account Key
- Storage account name
- Test Connection

Click:
```
Create
```

ğŸ‰ Your first Linked Service is created.

---

# ğŸ”„ What Just Happened?

You created a connection between:

Azure Data Factory â†’ Storage Account

This connection will be used in pipelines.

---

# ğŸ—ï¸ Step 8: Create First Pipeline (Empty Pipeline)

Go to:

âœï¸ **Author**

Click:

â• â†’ Pipeline â†’ Pipeline

Rename it:

```
pl_first_pipeline
```

You now see a blank canvas.

This is your workflow design area.

---

# ğŸ¯ Step 9: Add Copy Activity

From Activities panel (left side):

Drag:

```
Copy Data
```

Drop it onto canvas.

Click on the activity.

You will see 3 tabs:

- General
- Source
- Sink

---

# ğŸ“¥ Configure Source

In Source tab:

Click:
```
+ New Dataset
```

Choose:
- Azure Blob Storage
- CSV file

Create dataset:
```
ds_source_csv
```

---

# ğŸ“¤ Configure Sink

Go to Sink tab:

Click:
```
+ New Dataset
```

Choose:
- Azure Blob Storage
- CSV

Create:
```
ds_sink_csv
```

Now you have configured a simple copy activity.

---

# ğŸ§ª Step 10: Debug the Pipeline

Click:
```
Debug
```

Pipeline will execute immediately.

Go to:
ğŸ” **Monitor**

You can see:

- Pipeline run status
- Activity run details
- Execution time
- Error logs (if any)

---

# ğŸš€ Step 11: Publish Changes

Important:

ADF works in two modes:

- Development Mode
- Published Mode

Click:
```
Publish All
```

This deploys your changes to live version.

---

# ğŸ” Step 12: Add Trigger (Schedule)

To automate:

Click:
```
Add Trigger â†’ New/Edit
```

Choose:
- Schedule Trigger

Set:
- Start Date
- Time
- Recurrence (Daily)

Click:
```
OK
```

Now pipeline runs automatically.

---

# ğŸŒ³ Optional: Configure Git Integration (Recommended)

Professional projects always use Git.

Go to:

ğŸ§° Manage â†’ Git Configuration

You can connect:

- Azure DevOps
- GitHub

Benefits:

- Version control
- Branching
- Collaboration
- CI/CD support

---

# ğŸ” Monitor Section Explained

Go to:

ğŸ” Monitor

Here you can see:

- Pipeline runs
- Trigger runs
- Activity runs
- Failed runs
- Duration
- Error messages

This is your operational dashboard.

---

# ğŸ§  Beginner Analogy

Think of ADF Studio like:

- Author = Design room
- Manage = Connection room
- Monitor = Control room
- Publish = Deploy button

---

# ğŸ“Œ Common Beginner Mistakes

âŒ Forgetting to Publish  
âŒ Not testing connection  
âŒ Wrong region selection  
âŒ Not using resource groups properly  
âŒ Skipping Monitor checks  

---

# ğŸ What You Achieved in This Chapter

You successfully:

âœ”ï¸ Created Azure Data Factory  
âœ”ï¸ Launched ADF Studio  
âœ”ï¸ Explored UI  
âœ”ï¸ Created Linked Service  
âœ”ï¸ Created First Pipeline  
âœ”ï¸ Added Copy Activity  
âœ”ï¸ Debugged Pipeline  
âœ”ï¸ Published Changes  
âœ”ï¸ Configured Trigger  

You are now officially working with Azure Data Factory ğŸ‰

---

# ğŸš€ Coming Next in Chapter 3

In the next chapter, we will build:

ğŸ‘‰ Real Copy Pipeline (SQL â†’ Blob)  
ğŸ‘‰ Parameterized Pipeline  
ğŸ‘‰ Dynamic File Names  
ğŸ‘‰ Error Handling Basics  

---

# ğŸ“ Final Summary

Creating Azure Data Factory involves:

1. Creating resource in Azure Portal
2. Launching Studio
3. Creating Linked Services
4. Designing Pipelines
5. Debugging
6. Publishing
7. Monitoring

You now have the foundation to start building real-world ETL pipelines.

---

âœ¨ Congratulations! You have completed Chapter 2 â€“ Creating Azure Data Factory.
