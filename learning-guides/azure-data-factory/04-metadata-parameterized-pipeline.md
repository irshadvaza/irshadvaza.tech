# ğŸš€ Chapter 4: Metadata-Driven & Parameterized Pipeline (Enterprise Design)

---

# ğŸ¯ Objective of This Chapter

In this chapter, you will learn:

- What is Metadata-Driven Design
- Why enterprises use parameterized pipelines
- How to use Pipeline Parameters
- How to use Dynamic Content
- How to use Get Metadata Activity
- How to retrieve:
  - Exists
  - Item Count
  - Child Items
  - Last Modified
- How to build one pipeline for multiple files

By the end of this chapter, you will move from beginner to **enterprise-level ADF design**.

---

# ğŸ§  What is Metadata-Driven Pipeline?

Instead of creating:

âŒ 10 pipelines for 10 tables  
âŒ 20 pipelines for 20 files  

We create:

âœ… 1 Generic Pipeline  
âœ… Controlled by Parameters  
âœ… Driven by Metadata  

This is how large enterprises design scalable systems.

---

# ğŸ¢ Real Enterprise Scenario

Imagine you have Data Lake folder:

```
raw/input/
    product.csv
    customer.csv
    sales.csv
    inventory.csv
```

Instead of hardcoding file namesâ€¦

We design a dynamic pipeline that:

- Checks if file exists
- Gets file count
- Reads metadata
- Processes dynamically

---

# ğŸ“Œ Step 1: Create Parameterized Pipeline

Go to:

Author â†’ + â†’ Pipeline

Rename:

```
pl_metadata_driven_ingestion
```

---

# âš™ï¸ Step 2: Create Pipeline Parameters

Click on blank canvas â†’ Parameters tab

Create parameters:

| Name | Type | Default Value |
|------|------|---------------|
| p_folder_path | String | raw/input |
| p_file_name | String | product.csv |

Now this pipeline can accept different file names dynamically.

---

# ğŸ“‚ Step 3: Create Dataset with Parameters

Create new Dataset:

Azure Data Lake Storage Gen2 â†’ Delimited Text

In dataset settings:

Create parameters:

| Name | Type |
|------|------|
| folderPath | String |
| fileName | String |

In Connection tab:

Folder path:
```
@dataset().folderPath
```

File name:
```
@dataset().fileName
```

Name dataset:

```
ds_adls_dynamic_file
```

Now dataset is dynamic.

---

# ğŸ”„ Step 4: Pass Pipeline Parameters to Dataset

Inside pipeline:

Add Copy Activity (optional for later).

For dataset configuration:

Folder Path:
```
@pipeline().parameters.p_folder_path
```

File Name:
```
@pipeline().parameters.p_file_name
```

Now pipeline controls dataset.

---

# ğŸ” Step 5: Add Get Metadata Activity (Important)

From Activities panel:

Drag:

```
Get Metadata
```

Rename:

```
get_file_metadata
```

Connect it with dataset:

`ds_adls_dynamic_file`

---

# ğŸ“‹ Step 6: Configure Get Metadata Fields

Click on Get Metadata activity.

In Field List, select:

âœ”ï¸ Exists  
âœ”ï¸ Item Name  
âœ”ï¸ Item Type  
âœ”ï¸ Size  
âœ”ï¸ Last Modified  
âœ”ï¸ Child Items  
âœ”ï¸ Item Count  

Now this activity will retrieve metadata details.

---

# ğŸ§ª Practical Scenario 1: Check If File Exists

After Get Metadata:

Add **If Condition Activity**

Expression:

```
@activity('get_file_metadata').output.exists
```

If TRUE:
- Continue processing

If FALSE:
- Send alert
- Fail pipeline
- Log message

This prevents pipeline failure due to missing file.

---

# ğŸ“Š Practical Scenario 2: Get Item Count

If dataset points to folder:

Example:
```
raw/input/
```

Then select:

âœ”ï¸ Child Items  
âœ”ï¸ Item Count  

Output will show:

```
"itemCount": 4
```

Use expression:

```
@activity('get_file_metadata').output.itemCount
```

This helps in dynamic looping.

---

# ğŸ” Step 7: Process Multiple Files Using ForEach

After Get Metadata:

Add:

```
ForEach Activity
```

Items:

```
@activity('get_file_metadata').output.childItems
```

Inside ForEach:

Add Copy Activity.

For dynamic file name:

```
@item().name
```

Now pipeline will loop through all files automatically.

---

# ğŸ§ª Practical Scenario 3: Your product.csv Testing

You mentioned you:

- Dragged Get Metadata
- Connected dataset
- Set folder input
- File name: product.csv
- Tested fields like:
  - Exists
  - Child Items
  - Count

Hereâ€™s what happens internally:

If you specify:

Folder: raw/input  
File: product.csv  

And select:

âœ”ï¸ Exists  

Output:
```
{
  "exists": true
}
```

If you select:

âœ”ï¸ Size  

Output:
```
{
  "size": 2458
}
```

If you select:

âœ”ï¸ Last Modified  

Output:
```
{
  "lastModified": "2026-02-22T08:45:12Z"
}
```

If dataset is folder only and you select:

âœ”ï¸ Child Items  

Output:
```
{
  "childItems": [
    {"name": "product.csv"},
    {"name": "customer.csv"},
    {"name": "sales.csv"}
  ]
}
```

This is powerful for automation.

---

# ğŸ”¥ Step 8: Full Enterprise Metadata Flow

```
Get Metadata (Check Folder)
        â†“
If Condition (Exists?)
        â†“
ForEach (Loop Files)
        â†“
Copy Activity
        â†“
Store in Processed Layer
```

Single pipeline handles unlimited files.

---

# ğŸ“ˆ Advanced Enterprise Pattern: Table-Driven Metadata

Instead of hardcoding parametersâ€¦

Create control table in SQL:

```
TableName | SourceFolder | FileName | IsActive
------------------------------------------------
product   | raw/input    | product.csv | 1
customer  | raw/input    | customer.csv | 1
```

Pipeline reads control table â†’ processes dynamically.

This is called:

**Metadata-Driven Architecture**

---

# ğŸ§  Why Enterprises Love This Design

âœ”ï¸ Scalable  
âœ”ï¸ Maintainable  
âœ”ï¸ No duplicate pipelines  
âœ”ï¸ Centralized configuration  
âœ”ï¸ Easy onboarding for new tables  
âœ”ï¸ Reduced cost  

---

# ğŸ§ª Debugging Metadata Activity

Click:
```
Debug
```

Go to:
Monitor â†’ Activity Output

Check:

- exists
- size
- itemCount
- childItems

Always validate output JSON before using expressions.

---

# âŒ Common Mistakes

âŒ Forgetting to parameterize dataset  
âŒ Using wrong dynamic expression  
âŒ Not checking exists before processing  
âŒ Hardcoding file names  
âŒ Not handling empty folders  

---

# ğŸ—ï¸ Enterprise Folder Structure Example

```
datalake/
    raw/
    processed/
    curated/
```

Pipeline should:

- Ingest to raw
- Transform to processed
- Publish to curated

---

# ğŸ What You Achieved

You built:

âœ”ï¸ Parameterized Pipeline  
âœ”ï¸ Dynamic Dataset  
âœ”ï¸ Get Metadata Implementation  
âœ”ï¸ Exists Check  
âœ”ï¸ Item Count Retrieval  
âœ”ï¸ Child Item Looping  
âœ”ï¸ Enterprise-Ready Architecture  

This is how large-scale ADF systems are designed.

---

# ğŸš€ Coming Next in Chapter 5

Next chapter:

- Watermark Incremental Load
- Last Modified Based Load
- Delta Load Strategy
- Handling Updates
- Production Error Handling

You are now moving toward **ADF Solution Architect Level**.

---

# ğŸ“ Final Summary

Metadata-driven pipeline design allows you to:

- Control pipelines dynamically
- Reduce duplication
- Scale ingestion
- Automate file discovery
- Handle enterprise complexity

Instead of building multiple pipelinesâ€¦

You build one intelligent pipeline.

---

âœ¨ Congratulations! You have completed Chapter 4 â€“ Metadata-Driven & Parameterized Pipeline.
