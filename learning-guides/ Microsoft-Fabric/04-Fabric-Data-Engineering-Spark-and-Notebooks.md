ğŸ“˜ Chapter 04 â€“ Fabric Data Engineering (Spark & Notebooks Deep Dive)

From Beginner to Enterprise Data Engineer

ğŸš€ 1ï¸âƒ£ Introduction to Fabric Data Engineering

Inside Microsoft Fabric, Data Engineering is powered by:

Apache Spark (fully managed, SaaS, auto-scaled)

You do NOT:

Manage clusters

Configure infrastructure

Handle scaling manually

Fabric manages compute automatically.

ğŸ§  2ï¸âƒ£ What is Apache Spark (Simple Explanation)?

Apache Spark is a distributed engine that:

Processes large datasets

Runs parallel operations

Supports Python, SQL, Scala

Imagine:

Instead of 1 computer processing 1 million rows,
Spark uses multiple executors in parallel.

1M rows
 â†“
Split into chunks
 â†“
Processed simultaneously
 â†“
Results combined


Fabric runs Spark behind the scenes.

ğŸ— 3ï¸âƒ£ Spark Architecture in Fabric
User Notebook
     â†“
Spark Driver
     â†“
Executors (Parallel Workers)
     â†“
OneLake (Storage)


Storage layer is:

OneLake

Everything reads and writes to Delta tables in OneLake.

ğŸ§ª 4ï¸âƒ£ Your First Fabric Notebook (Step-by-Step)
ğŸ¯ Scenario: Retail Analytics Project

We have:

sales.csv (1M rows)

customers.csv

products.csv

Goal:

Create analytics-ready Gold table.

ğŸŸ¢ Step 1 â€“ Create Notebook

Inside Lakehouse:

New â†’ Notebook â†’ Name: Retail_Data_Engineering


Select language: Python

ğŸŸ¢ Step 2 â€“ Load Raw Data (Bronze Layer)
df_sales = spark.read.csv("Files/sales.csv", header=True, inferSchema=True)

df_sales.display()


Now Spark loads file from OneLake.

ğŸŸ¢ Step 3 â€“ Basic Transformations

Remove invalid revenue:

df_clean = df_sales.filter(df_sales["revenue"] > 0)


Add calculated column:

from pyspark.sql.functions import col

df_clean = df_clean.withColumn("profit", col("revenue") - col("cost"))

ğŸŸ¢ Step 4 â€“ Save as Delta Table (Silver Layer)
df_clean.write.format("delta").mode("overwrite").saveAsTable("sales_clean")


Now visible in:

Lakehouse â†’ Tables â†’ sales_clean

Stored as Delta in OneLake.

ğŸŸ¢ Step 5 â€“ Create Gold Layer Aggregation
from pyspark.sql.functions import sum

df_gold = df_clean.groupBy("region") \
                  .agg(sum("revenue").alias("total_revenue"),
                       sum("profit").alias("total_profit"))

df_gold.write.format("delta").mode("overwrite").saveAsTable("sales_summary")


Now ready for Power BI.

âš¡ 5ï¸âƒ£ Understanding Delta Lake in Fabric

Fabric stores tables in:

Open Delta Lake format

Benefits:

Feature	Why Important
ACID Transactions	No partial writes
Time Travel	Query older versions
Schema Evolution	Add columns safely
Fast reads	Optimized Parquet

Example:

SELECT * FROM sales_clean VERSION AS OF 1;

ğŸ”„ 6ï¸âƒ£ Partitioning Strategy (Performance Tuning)

If table has millions of rows:

Partition by:

Date

Region

Category

Example:

df_clean.write.format("delta") \
    .partitionBy("region") \
    .mode("overwrite") \
    .saveAsTable("sales_partitioned")


Why?

Instead of scanning full table,
Spark scans only required partition.

Huge performance boost.

ğŸ§¬ 7ï¸âƒ£ Medallion Architecture with Spark
Bronze  â†’ Raw data
Silver  â†’ Cleaned & validated
Gold    â†’ Aggregated business layer


Example mapping:

Layer	Table
Bronze	sales_raw
Silver	sales_clean
Gold	sales_summary
ğŸ“Š 8ï¸âƒ£ Working with Large Dataset (1M+ Rows Example)

Simulate 1M rows:

from pyspark.sql.functions import rand

df_big = spark.range(0, 1000000) \
    .withColumn("revenue", rand()*1000) \
    .withColumn("cost", rand()*500)

df_big.write.format("delta").saveAsTable("big_sales")


Spark processes this in seconds.

Traditional SQL server may struggle.

ğŸ” 9ï¸âƒ£ Spark SQL in Fabric

You can use SQL inside notebook:

%%sql

SELECT region, SUM(revenue)
FROM sales_clean
GROUP BY region;


Spark + SQL flexibility.

ğŸš€ ğŸ”Ÿ Caching for Performance

If dataset reused multiple times:

df_clean.cache()


Why?

Keeps data in memory

Avoids recomputation

Faster execution

ğŸ§ª 11ï¸âƒ£ Handling Null Values
df_clean = df_clean.fillna(0)


Or specific column:

df_clean = df_clean.fillna({"revenue":0})

ğŸ” 12ï¸âƒ£ Incremental Load Strategy

Instead of full reload:

df_new = spark.read.csv("Files/new_sales.csv", header=True)

df_new.write.format("delta") \
    .mode("append") \
    .saveAsTable("sales_clean")


Efficient for enterprise pipelines.

ğŸ§  13ï¸âƒ£ Real Enterprise Use Case
Smart City Traffic Monitoring

Data:

5M sensor records daily

Vehicle speed

Location

Timestamp

Pipeline:

Ingest raw sensor data (Bronze)

Remove corrupted records (Silver)

Aggregate hourly congestion stats (Gold)

Power BI real-time dashboard

ML prediction model

All inside Fabric.

ğŸ” 14ï¸âƒ£ Security in Spark Layer

Uses:

Microsoft Entra ID

Supports:

Table-level security

Row-level security

Workspace roles

Example:

Only Finance role can query profit columns.

ğŸ 15ï¸âƒ£ Optimization Techniques
Technique	Benefit
Partitioning	Faster filtering
Caching	Reuse data
Z-Ordering	Faster selective queries
Incremental loads	Lower cost
Proper schema	Avoid inferSchema overhead
ğŸ¯ 16ï¸âƒ£ Spark vs Traditional SQL Server
SQL Server	Spark
Single machine	Distributed
Limited scale	Massive scale
Expensive scaling	Elastic
Structured only	Structured + Semi-structured
ğŸ“Š 17ï¸âƒ£ End-to-End Engineering Flow
Raw CSV
   â†“
Spark Load
   â†“
Clean & Transform
   â†“
Save Delta Table
   â†“
Aggregate
   â†“
Power BI Direct Lake


Direct integration with:

Power BI

No data duplication required.

ğŸ§‘â€ğŸ’¼ 18ï¸âƒ£ Interview-Level Understanding

If asked:

â“ What is Fabric Data Engineering?

Answer:

Fabric Data Engineering is a fully managed Spark-based big data processing environment integrated with OneLake, enabling scalable transformation, optimization, and Delta Lake management within Microsoft Fabric.

ğŸ† 19ï¸âƒ£ Why Fabric Spark is Powerful?

No cluster management

Unified storage

Native Delta

Direct BI integration

Enterprise security

SaaS simplicity

ğŸ“Œ 20ï¸âƒ£ Key Takeaways

You now understand:

Spark basics

Notebook creation

Delta Lake

Partitioning

Performance tuning

Incremental loads

Enterprise architecture

You are now thinking like:

ğŸ“ Fabric Data Engineer & Architect

ğŸ“š Next Chapter
