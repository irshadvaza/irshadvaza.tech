ğŸ“˜ Chapter 03 â€“ Fabric Data Factory (Deep Dive)

Enterprise Data Integration from Beginner to Architect Level

ğŸš€ 1ï¸âƒ£ Introduction to Fabric Data Factory

Microsoft Fabric includes a fully integrated data integration engine called:

Data Factory in Fabric

It is the evolution of:

Azure Data Factory

But now:

Fully SaaS

Integrated with OneLake

No linked services complexity

No IR management

Unified security

ğŸ§  2ï¸âƒ£ Why Data Factory is Important?

In real projects, data comes from:

ERP systems

CRM systems

APIs

CSV files

Databases

Streaming systems

Before analytics, data must be:

Extracted

Cleaned

Transformed

Loaded

This process is called:

ETL / ELT

Fabric Data Factory handles this end-to-end.

ğŸ— 3ï¸âƒ£ Core Components of Fabric Data Factory
Component	Purpose
Pipelines	Orchestration
Copy Activity	Data movement
Dataflow Gen2	Low-code transformation
Notebook Activity	Spark processing
Scheduled Triggers	Automation
ğŸ§© 4ï¸âƒ£ Architecture Overview
Source Systems
     â†“
Copy Activity
     â†“
OneLake (Bronze)
     â†“
Dataflow / Notebook
     â†“
Silver / Gold Tables
     â†“
Power BI


Everything writes to:

OneLake

ğŸ§ª 5ï¸âƒ£ Step-by-Step Real Enterprise Example
ğŸ¯ Scenario: Retail Company

Data Sources:

SQL Server (Sales DB)

CSV files (Products)

REST API (Exchange rates)

Goal:

Create unified reporting dataset.

ğŸŸ¢ Step 1 â€“ Create Pipeline

Inside Workspace:

New â†’ Data Pipeline â†’ Name: Retail_ETL_Pipeline

ğŸŸ¢ Step 2 â€“ Add Copy Activity (SQL to Lakehouse)

Drag Copy Data activity.

Source:

SQL Server

Table: Sales

Destination:

Lakehouse â†’ Bronze layer

Now pipeline looks like:

[Copy Sales SQL â†’ Bronze Table]

ğŸŸ¢ Step 3 â€“ Copy CSV Files

Add another Copy Activity.

Source:

Upload folder / Blob

Destination:

Lakehouse Files â†’ Bronze/products_raw

ğŸŸ¢ Step 4 â€“ Use Dataflow Gen2 (Transformation Layer)

What is Dataflow Gen2?

Low-code transformation tool inside Fabric.

You can:

Remove nulls

Change data types

Join tables

Aggregate data

Example transformation:

Sales + Products â†’ Join on product_id
Filter revenue > 0
Add calculated column: profit = revenue - cost


Save output as:

sales_clean (Silver layer)

ğŸŸ¢ Step 5 â€“ Notebook for Advanced Logic

Add Notebook Activity.

Example:

df = spark.read.table("sales_clean")

df_gold = df.groupBy("region") \
            .sum("revenue") \
            .withColumnRenamed("sum(revenue)", "total_revenue")

df_gold.write.format("delta").mode("overwrite").saveAsTable("sales_summary")


Now you created:

Gold Layer table

ğŸŸ¢ Step 6 â€“ Add Trigger (Automation)

Set schedule:

Daily at 2:00 AM


Pipeline now runs automatically.

ğŸ¯ 6ï¸âƒ£ Medallion Architecture in Data Factory

Fabric follows:

Bronze â†’ Silver â†’ Gold

ğŸŸ¤ Bronze

Raw ingestion
No transformation

Example:

sales_raw

products_raw

âšª Silver

Cleaned and validated

Example:

sales_clean

products_clean

ğŸŸ¡ Gold

Business-ready

Example:

sales_summary

region_performance

âš¡ 7ï¸âƒ£ ELT vs ETL in Fabric

Traditional ETL:

Transform â†’ Load


Fabric approach (ELT):

Load â†’ Transform in Lakehouse


Because Fabric storage is powerful and scalable.

ğŸ§¬ 8ï¸âƒ£ Dataflow Gen2 Deep Explanation

Dataflow Gen2 is built on Power Query engine.

Supports:

300+ connectors

Visual transformations

Reusable logic

Incremental refresh

Example Transformations:

Transformation	Example
Filter	Remove negative revenue
Join	Sales + Customer
Group By	Sum revenue by region
Add Column	profit margin
ğŸ¢ 9ï¸âƒ£ Real Enterprise Scenario (Smart City)

City collects:

Traffic sensor data

Pollution data

Weather data

Public transport logs

Pipeline flow:

Copy sensor data (Bronze)

Clean invalid records (Silver)

Aggregate hourly stats (Gold)

Power BI dashboard

ML model forecasting

All within Fabric.

ğŸ”„ ğŸ”Ÿ Monitoring & Debugging

Fabric provides:

Pipeline run history

Error logs

Duration tracking

Dependency view

Example:

If Copy Activity fails:

View error message

Retry activity

Enable alerts

ğŸ” 11ï¸âƒ£ Security & Governance

Uses:

Microsoft Entra ID

Supports:

Role-based access

Workspace permissions

Data masking

Sensitivity labels

Example:

Finance team can access Gold
Engineering team can access Bronze

ğŸ’° 12ï¸âƒ£ Cost Optimization Strategy

Best Practices:

Use incremental loads

Avoid full reload

Use partitioned tables

Monitor capacity usage

Fabric runs on capacity model (F SKU).

ğŸ“Š 13ï¸âƒ£ Complete End-to-End Flow Diagram
SQL Server
CSV Files
API Data
    â†“
Copy Activity
    â†“
Bronze Tables (Raw)
    â†“
Dataflow Gen2
    â†“
Silver Tables
    â†“
Notebook (Spark)
    â†“
Gold Tables
    â†“
Power BI Direct Lake

ğŸ§  14ï¸âƒ£ Interview-Level Understanding

If asked:

â“ What is Fabric Data Factory?

Answer:

Fabric Data Factory is a fully managed SaaS data integration engine inside Microsoft Fabric that enables orchestration, ingestion, and transformation of enterprise data into OneLake using pipelines, Dataflow Gen2, and Spark notebooks.

ğŸ¯ 15ï¸âƒ£ Key Advantages Over Azure Data Factory
Azure Data Factory	Fabric Data Factory
Separate service	Integrated
Linked services	Simplified
External storage	OneLake native
Infra management	Fully SaaS
ğŸ† 16ï¸âƒ£ What Makes Fabric Data Factory Unique?

Native Lakehouse integration

No storage configuration

Shared security model

Direct Power BI connectivity

Built-in Spark support

ğŸ“Œ 17ï¸âƒ£ Key Takeaways

You now understand:

Pipelines

Copy activity

Dataflow Gen2

Notebook orchestration

Triggers

Medallion architecture

Enterprise ETL design

You are now thinking like:

ğŸ“ Fabric Data Engineer

ğŸ“š Next Chapter
