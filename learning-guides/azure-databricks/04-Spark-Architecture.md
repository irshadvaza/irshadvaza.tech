ğŸ“˜ Chapter 4 â€“ Apache Spark Architecture

Understanding How Spark Executes Your Code

ğŸš€ 1ï¸âƒ£ Introduction

In the previous chapter, we learned:

What Apache Spark is

Why it is fast

Where it is used

Now the most important question:

What happens internally when you run Spark code?

This chapter explains how Spark works behind the scenes â€” step by step.

This is one of the most important topics for interviews.

ğŸ§  2ï¸âƒ£ High-Level Spark Architecture (One-Line View)

When you run Spark code:

User Code â†’ Driver â†’ Cluster Manager â†’ Executors â†’ Storage


Or visually:

          Driver Program
                |
     -------------------------
     |           |           |
 Executor     Executor     Executor
 (Worker)     (Worker)     (Worker)


Spark follows a Masterâ€“Worker architecture.

Driver = Master

Executors = Workers

ğŸ§© 3ï¸âƒ£ Core Components of Spark Architecture

Spark architecture has 5 main components:

Driver Program

Cluster Manager

Worker Nodes

Executors

Tasks

Letâ€™s understand each one clearly.

ğŸ§  4ï¸âƒ£ Driver Program (The Brain of Spark)

The Driver is the main controller of a Spark application.

It is responsible for:

Converting user code into execution plan

Creating DAG (Directed Acyclic Graph)

Dividing jobs into stages

Sending tasks to executors

Collecting results

ğŸ“Œ Example

You write:

df.groupBy("country").count()


The Driver:

Analyzes the query

Creates execution plan

Breaks it into stages

Sends tasks to executors

The driver does NOT process large data.
It only controls execution.

ğŸ–¥ 5ï¸âƒ£ Cluster Manager (Resource Controller)

The Cluster Manager manages resources in the cluster.

It decides:

How many executors to allocate

How much memory to assign

Where to run the executors

Common Cluster Managers
Cluster Manager	Used In
Standalone	Spark built-in
YARN	Hadoop ecosystem
Kubernetes	Container-based systems
Databricks Runtime	Managed environment
ğŸ­ 6ï¸âƒ£ Worker Nodes

Worker nodes are machines in the cluster.

Each worker node:

Hosts one or more executors

Performs actual data processing

Communicates with driver

Think of workers as physical/virtual machines.

âš™ 7ï¸âƒ£ Executors (The Muscle ğŸ’ª)

Executors are processes that run on worker nodes.

They are responsible for:

Executing tasks

Performing transformations

Reading & writing data

Storing intermediate data in memory

Each executor has:

CPU cores

Memory

Task slots

Important Point

Executors are created when a Spark application starts
Executors die when application ends

ğŸ§µ 8ï¸âƒ£ What is a Task?

A task is the smallest unit of work in Spark.

Example:

If a dataset has 4 partitions â†’ Spark creates 4 tasks.

Each task processes one partition.

ğŸ”„ 9ï¸âƒ£ Complete Execution Flow (Step-by-Step)

Letâ€™s understand what happens when you run a Spark job.

Step 1ï¸âƒ£: User Submits Code

Example:

df.filter("amount > 1000").groupBy("region").sum()

Step 2ï¸âƒ£: Driver Creates Logical Plan

Spark analyzes the query.

It creates:

Logical plan

Optimized plan

This is done by Catalyst Optimizer.

Step 3ï¸âƒ£: DAG Creation

Spark creates a:

Directed Acyclic Graph (DAG)

DAG represents the sequence of transformations.

Step 4ï¸âƒ£: Job â†’ Stage â†’ Task Breakdown

Spark divides execution into:

Job

Stages

Tasks

ğŸ”¹ Job

Triggered by an action (count, collect, write)

ğŸ”¹ Stage

Group of transformations without shuffle

ğŸ”¹ Task

Smallest unit of execution (per partition)

Step 5ï¸âƒ£: Cluster Manager Allocates Resources

Cluster Manager provides:

Executors

CPU

Memory

Step 6ï¸âƒ£: Tasks Sent to Executors

Driver sends tasks.

Executors:

Process data

Return results

Step 7ï¸âƒ£: Final Output Written

Data is written to:

ADLS

S3

Delta Lake

Database

ğŸ“Š 1ï¸âƒ£0ï¸âƒ£ Job, Stage, and Task (Very Important)

Understanding this is critical.

Application
   â†“
Job
   â†“
Stage
   â†“
Task

Example Scenario

You run:

df.groupBy("country").count().show()


Spark creates:

1 Job

2 Stages (because of shuffle)

Multiple Tasks (based on partitions)

ğŸ”€ 1ï¸âƒ£1ï¸âƒ£ What is Shuffle?

Shuffle occurs when data moves across partitions.

Example:

groupBy

join

distinct

orderBy

Shuffle is expensive because:

Data moves across network

Disk I/O increases

Minimizing shuffle improves performance.

ğŸ§± 1ï¸âƒ£2ï¸âƒ£ Spark Architecture in Databricks

In Databricks:

Notebook â†’ Driver Node â†’ Worker Nodes â†’ Delta Lake (ADLS)


Driver runs in driver node

Executors run in worker nodes

Data stored in ADLS

Important:

Compute and Storage are separated

ğŸ” 1ï¸âƒ£3ï¸âƒ£ Fault Tolerance in Spark

Spark is fault tolerant because of:

RDD Lineage

If a partition is lost:

Spark recomputes it using lineage graph

No manual recovery required

âš¡ 1ï¸âƒ£4ï¸âƒ£ Parallelism in Spark

Parallelism depends on:

Number of partitions

Number of executor cores

More partitions â†’ More tasks â†’ More parallel execution

But too many partitions â†’ Overhead

Balance is important.

ğŸ§  1ï¸âƒ£5ï¸âƒ£ Important Spark Concepts Summary
Concept	Meaning
Driver	Controls execution
Executor	Executes tasks
Cluster Manager	Allocates resources
DAG	Execution plan
Job	Triggered by action
Stage	Group of tasks
Task	Smallest unit of work
Shuffle	Data movement across partitions
