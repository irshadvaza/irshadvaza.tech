# ğŸ¥ K-Nearest Neighbors (KNN)

KNN is a **simple machine learning algorithm** used for classification.  
It predicts the **class of a new data point** based on the **K closest neighbors** in the dataset.  

---

## ğŸ“˜ What is KNN?

- **Full Name:** K-Nearest Neighbors  
- **Type:** Lazy learner, instance-based algorithm  
- **Idea:** Similar points belong to the same class  
- **Prediction:** Based on the **majority vote** of K nearest neighbors  

> KNN **does not train a model**; it stores the dataset and predicts based on neighbors.

---

## ğŸ§  Real-Life Analogy

Imagine moving to a new city and asking your **5 nearest neighbors** if the neighborhood is safe:  

- âœ… 4 say safe  
- âŒ 1 says unsafe  

You conclude â†’ **Safe**  

> KNN works the same way using **majority vote** from the closest points.

---

## ğŸ¯ KNN in Diabetes Prediction

We want to predict **whether a patient has diabetes**.  

**Steps KNN follows:**

1. Take a **new patient**  
2. Measure **distance** to all other patients  
3. Select **K closest patients**  
4. Check **majority class**  
5. Predict patientâ€™s class  

---

## ğŸ”¢ Understanding K

- K = number of neighbors considered for prediction  

**Example:**

| K | Neighbors | Class Distribution | Prediction |
|---|-----------|-----------------|------------|
| 3 | 2 diabetic, 1 non-diabetic | Majority = Diabetic | âœ… Diabetic |
| 5 | 3 non-diabetic, 2 diabetic | Majority = Non-diabetic | âŒ Non-diabetic |

> Choosing K is important:  
> - Small K â†’ sensitive to noise (may overfit)  
> - Large K â†’ smoother predictions (may underfit)

---

## ğŸ“ Distance Metrics

| Metric | Description | Formula |
|--------|-------------|---------|
| Euclidean | Straight-line distance | âˆš((x1-x2)Â² + (y1-y2)Â²) |
| Manhattan | Grid-like distance | |x1-x2| + |y1-y2| |

---

## ğŸ’¡ Why Feature Scaling is Important

Example:

- Glucose = 150  
- BMI = 30  

Without scaling, **Glucose dominates distance**, misleading results.  

**Solution:** Use **StandardScaler** to normalize all features.

---

## ğŸ“¦ Advantages of KNN

- âœ… Simple and intuitive  
- âœ… Easy to implement  
- âœ… No training time (lazy learner)  

---

## âš  Disadvantages of KNN

- âŒ Slow for large datasets  
- âŒ Sensitive to feature scaling  
- âŒ Sensitive to irrelevant features  

---

## ğŸ”¹ Example â€“ Diabetes Dataset

If K = 5:

- Checks **5 nearest points**  
- Majority vote decides class  

```text
Neighbors: 1 1 0 1 0
Class Labels: Diabetic=1, Non-diabetic=0
Majority = 1 â†’ Predict Diabetic âœ…
```


```
from sklearn.neighbors import KNeighborsClassifier

# Create KNN model
model = KNeighborsClassifier()

# Hyperparameter grid
param_grid = {
    'n_neighbors': [3,5,7,9,11],
    'weights': ['uniform', 'distance'],
    'metric': ['euclidean', 'manhattan']
}
```

Explanation:

n_neighbors â†’ number of neighbors to consider

weights â†’ 'uniform' (all neighbors equal) or 'distance' (closer neighbors weigh more)

metric â†’ distance calculation method

ğŸ“Š Visual Intuition

```
Original Dataset: 0 0 0 1 1
New Point:        ?
Neighbors: 1 1 0 1 0
Majority = 1 â†’ Predict Diabetic âœ…
```

```
ğŸ”¹ Detailed GridSearchCV Hyperparameters

1ï¸âƒ£ n_neighbors

'n_neighbors': [3,5,7,9,11]
```

Try different K values:
â€¢ 3 neighbors
â€¢ 5 neighbors
â€¢ 7 neighbors
â€¢ 9 neighbors
â€¢ 11 neighbors

Why?
Because we donâ€™t know which K gives the best performance.

Small K:
â€¢ More sensitive
â€¢ May overfit

Large K:
â€¢ More stable
â€¢ May underfit

So we test multiple values to find the optimal K.

2ï¸âƒ£ weights

'weights': ['uniform', 'distance']


uniform: All neighbors have equal importance.

Example: If K=5, each neighbor has same vote.

distance: Closer neighbors have more importance.

Example: The nearest patient influences prediction more.

Usually performs better than uniform weighting.

3ï¸âƒ£ metric

'metric': ['euclidean', 'manhattan']


Defines how distance is calculated between points.

Euclidean: straight-line distance

Manhattan: grid-based distance
'''

```
3ï¸âƒ£ GridSearchCV with RepeatedStratifiedKFold

We are doing two things:

1ï¸âƒ£ Cross-validation â†’ RepeatedStratifiedKFold
2ï¸âƒ£ Hyperparameter tuning â†’ GridSearchCV

ğŸ”¹ Importing the Tools
from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold


GridSearchCV: Finds the best parameter combination

RepeatedStratifiedKFold: Performs robust cross-validation

ğŸ”¹ Setting up Cross-Validation
cv = RepeatedStratifiedKFold(
    n_splits=5,
    n_repeats=3,
    random_state=42
)


K-Fold: Dataset split into 5 parts â†’ each used once as validation

Stratified: Each fold maintains class ratio (important for imbalanced datasets)

Repeated: 5-fold process repeated 3 times â†’ more reliable results

ğŸ”¹ GridSearchCV
grid = GridSearchCV(
    model,
    param_grid,
    cv=cv,
    scoring='roc_auc',
    n_jobs=-1
)


Tries all combinations in param_grid

Uses cross-validation to evaluate each combination

Selects the best parameters

ğŸ”¹ Hyperparameters Explained
1ï¸âƒ£ n_neighbors

[3,5,7,9,11] â†’ Try different K values

Small K â†’ sensitive, may overfit

Large K â†’ stable, may underfit

2ï¸âƒ£ weights

'uniform' â†’ all neighbors equal

'distance' â†’ closer neighbors weigh more (usually better)

3ï¸âƒ£ metric

'euclidean' â†’ straight-line distance

'manhattan' â†’ grid-like distance

ğŸ”¹ Fitting GridSearch
grid.fit(X_train_res, y_train_res)

print("Best Parameters:", grid.best_params_)
print("Best CV Score:", grid.best_score_)


Example Output:

Best Parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean'}
Best CV Score: 0.83

âœ… Summary

RepeatedStratifiedKFold â†’ robust, stratified cross-validation

GridSearchCV â†’ systematic hyperparameter tuning

Together: finds best KNN parameters with reliable performance
