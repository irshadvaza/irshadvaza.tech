
ğŸ§ª Lab 01 â€“ Analyze Data with Apache Spark in Fabric
ğŸ¯ Lab Objective

In this lab, you will:

Create a Fabric workspace

Create a Lakehouse

Upload data files

Create a Notebook

Use PySpark to analyze data

Transform and save data

Create Delta tables

Run SQL queries

Visualize data using charts

â± Estimated Time

45 Minutes

ğŸ“Œ Prerequisites

Access to a Fabric-enabled tenant

Internet connection

Fabric login credentials

ğŸ— Part 1 â€“ Create Workspace

Open Fabric portal:
ğŸ‘‰ https://app.fabric.microsoft.com

Sign in.

From left menu, select Workspaces.

Click New Workspace.

Enter a name (example: SparkLabWorkspace).

In Advanced settings, choose a license mode:

Trial

Premium

Fabric capacity

Click Create.

âœ… Workspace is ready.

ğŸ Part 2 â€“ Create Lakehouse

In left menu, click Create
(If not visible, click the ... first.)

Under Data Engineering, select Lakehouse.

Enter a name:
SalesLakehouse

Make sure:

Lakehouse schemas (Public Preview) = Disabled


Click Create.

Fabric creates storage automatically in:

OneLake

ğŸ“¥ Part 3 â€“ Upload Data Files
Step 1 â€“ Download Data

Download dataset:

https://github.com/MicrosoftLearning/dp-data/raw/main/orders.zip


Extract the file.

You should see:

orders/
   â”œâ”€â”€ 2019.csv
   â”œâ”€â”€ 2020.csv
   â””â”€â”€ 2021.csv

Step 2 â€“ Upload to Lakehouse

Open your Lakehouse.

In Explorer pane, click ... next to Files

Select Upload â†’ Upload Folder

Choose the orders folder

Upload

Verify:

Files/
   â””â”€â”€ orders/
       â”œâ”€â”€ 2019.csv
       â”œâ”€â”€ 2020.csv
       â””â”€â”€ 2021.csv

ğŸ““ Part 4 â€“ Create Notebook

Click Create

Select Notebook

Rename it to:

Sales_Data_Exploration

Add Markdown Title

Convert first cell to Markdown.

Add:

# Sales Order Data Exploration
This notebook explores sales order data using PySpark.

ğŸ”¥ Part 5 â€“ Create Spark DataFrame
Load 2019 Data
df = spark.read.format("csv") \
    .option("header","false") \
    .load("Files/orders/2019.csv")

display(df)

Define Schema (Best Practice)
from pyspark.sql.types import *

orderSchema = StructType([
    StructField("SalesOrderNumber", StringType()),
    StructField("SalesOrderLineNumber", IntegerType()),
    StructField("OrderDate", DateType()),
    StructField("CustomerName", StringType()),
    StructField("Email", StringType()),
    StructField("Item", StringType()),
    StructField("Quantity", IntegerType()),
    StructField("UnitPrice", FloatType()),
    StructField("Tax", FloatType())
])

df = spark.read.format("csv") \
    .schema(orderSchema) \
    .load("Files/orders/*.csv")

display(df)


Now all 3 years are loaded.

ğŸ” Part 6 â€“ Explore Data
Filter Columns
customers = df.select("CustomerName", "Email")

print(customers.count())
print(customers.distinct().count())

display(customers.distinct())

Filter Specific Product
customers = df.select("CustomerName", "Email") \
              .where(df["Item"] == "Road-250 Red, 52")

display(customers.distinct())

ğŸ“Š Part 7 â€“ Aggregation
Quantity per Product
productSales = df.select("Item", "Quantity") \
                 .groupBy("Item") \
                 .sum()

display(productSales)

Orders per Year
from pyspark.sql.functions import *

yearlySales = df.select(year(col("OrderDate")).alias("Year")) \
                .groupBy("Year") \
                .count() \
                .orderBy("Year")

display(yearlySales)

ğŸ”„ Part 8 â€“ Transform Data
from pyspark.sql.functions import *

transformed_df = df.withColumn("Year", year(col("OrderDate"))) \
                   .withColumn("Month", month(col("OrderDate"))) \
                   .withColumn("FirstName", split(col("CustomerName"), " ").getItem(0)) \
                   .withColumn("LastName", split(col("CustomerName"), " ").getItem(1))

display(transformed_df.limit(5))

ğŸ’¾ Part 9 â€“ Save as Parquet
transformed_df.write.mode("overwrite") \
    .parquet("Files/transformed_data/orders")

print("Transformed data saved!")


Reload:

orders_df = spark.read.format("parquet") \
    .load("Files/transformed_data/orders")

display(orders_df)

ğŸš€ Part 10 â€“ Partition Data
orders_df.write.partitionBy("Year","Month") \
    .mode("overwrite") \
    .parquet("Files/partitioned_data")

print("Partitioned data saved!")


Load 2021 only:

orders_2021_df = spark.read.format("parquet") \
    .load("Files/partitioned_data/Year=2021/Month=*")

display(orders_2021_df)

ğŸ› Part 11 â€“ Create Delta Table
df.write.format("delta") \
    .saveAsTable("salesorders")


Check table:

spark.sql("DESCRIBE EXTENDED salesorders").show(truncate=False)

ğŸ§¾ Query Using SQL
%%sql
SELECT YEAR(OrderDate) AS OrderYear,
       SUM((UnitPrice * Quantity) + Tax) AS GrossRevenue
FROM salesorders
GROUP BY YEAR(OrderDate)
ORDER BY OrderYear;

ğŸ“ˆ Part 12 â€“ Visualize with Matplotlib
from matplotlib import pyplot as plt

df_sales = spark.sql("""
SELECT CAST(YEAR(OrderDate) AS STRING) AS OrderYear,
       SUM((UnitPrice * Quantity) + Tax) AS GrossRevenue
FROM salesorders
GROUP BY YEAR(OrderDate)
ORDER BY OrderYear
""").toPandas()

plt.figure(figsize=(8,4))
plt.bar(df_sales["OrderYear"], df_sales["GrossRevenue"])
plt.title("Revenue by Year")
plt.xlabel("Year")
plt.ylabel("Revenue")
plt.show()

ğŸ¨ Using Seaborn
import seaborn as sns
sns.set_theme(style="whitegrid")

sns.lineplot(x="OrderYear",
             y="GrossRevenue",
             data=df_sales)

plt.show()

ğŸ§  What You Learned

In this lab you:

âœ… Created workspace
âœ… Created Lakehouse
âœ… Uploaded files
âœ… Used PySpark
âœ… Filtered and grouped data
âœ… Transformed data
âœ… Saved Parquet files
âœ… Partitioned data
âœ… Created Delta table
âœ… Ran SQL queries
âœ… Built visualizations

ğŸ† You Now Understand

Spark DataFrames

Delta tables

Lakehouse storage

Partitioning strategy

SQL in notebooks

Basic data visualization
