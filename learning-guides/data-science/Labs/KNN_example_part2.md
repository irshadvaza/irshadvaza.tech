K-Nearest Neighbors (KNN)
ğŸ“˜ What is KNN? (Very Simple Explanation)

KNN = K-Nearest Neighbors

One of the simplest machine learning algorithms.

Works based on distance between data points.

ğŸ§  Simple Real-Life Example

Imagine:

You move to a new city and want to know if a neighborhood is safe.

You ask your 5 nearest neighbors.

If:

4 say it is safe

1 says it is not safe

You decide â†’ It is safe.

This is exactly how KNN works: it looks at nearby points and predicts based on majority vote.

ğŸ¯ In Our Diabetes Problem

We want to predict:

ğŸ‘‰ Does a patient have diabetes or not?

Steps KNN follows:

Take a new patient.

Measure distance to all other patients.

Select K closest patients.

Check majority class among neighbors.

Predict based on majority vote.

ğŸ”¢ What is K?

K = number of neighbors considered for prediction.

Example:

If K = 3, look at 3 closest patients:

2 diabetic

1 non-diabetic

Prediction = Diabetic

ğŸ“ How Distance is Measured?

Euclidean distance â†’ straight-line distance

Manhattan distance â†’ grid-based distance

ğŸ’¡ Why Scaling is Important?

If one feature is much larger than others, e.g.:

Glucose = 150

BMI = 30

Then distance is dominated by the larger feature â†’ misleading results

Solution: Use StandardScaler to normalize all features.

ğŸ“¦ Advantages of KNN

âœ” Simple to understand
âœ” Easy to implement
âœ” No training time (lazy learner)

âš  Disadvantages

âŒ Slow for large datasets
âŒ Sensitive to feature scaling
âŒ Sensitive to irrelevant features

ğŸ”¹ Example (Diabetes KNN)

If K = 5:

Checks 5 closest points

Majority vote decides the class

Example:

3 diabetic

2 non-diabetic

Prediction = Diabetic âœ…
