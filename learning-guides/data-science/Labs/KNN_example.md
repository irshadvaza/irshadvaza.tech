# ğŸ§ª KNN Classification â€“ Pima Indians Diabetes Dataset

## ğŸ“Œ Objective

The objective of this lab is to build a Machine Learning classification model using **K-Nearest Neighbors (KNN)** to predict whether a patient has diabetes.

This project demonstrates:

- Data preprocessing
- Handling imbalanced dataset
- Feature scaling
- Hyperparameter tuning
- Model evaluation using multiple metrics
- Visualization of ROC and PR curves

---

## ğŸ“Š Dataset Description

We are using the **Pima Indians Diabetes Dataset**.

This dataset contains medical information about female patients of Pima Indian heritage.

### ğŸ¯ Target Variable

| Value | Meaning |
|-------|---------|
| 0     | No Diabetes |
| 1     | Diabetes |

This is a **binary classification problem**.

---

## ğŸ“ Features

| Feature | Description |
|----------|-------------|
| Pregnancies | Number of pregnancies |
| Glucose | Plasma glucose concentration |
| BloodPressure | Diastolic blood pressure |
| SkinThickness | Triceps skin fold thickness |
| Insulin | 2-Hour serum insulin |
| BMI | Body Mass Index |
| DiabetesPedigreeFunction | Genetic influence factor |
| Age | Age of patient |
| Outcome | Target variable |

---

## ğŸ§  Learning Goals

After completing this lab, you should understand:

- How KNN works
- Why scaling is important for distance-based models
- What is class imbalance
- How SMOTE helps balance data
- How GridSearch finds best hyperparameters
- How to evaluate classification models properly



# ğŸ“¦ Step 2 â€“ Import Libraries & Load Dataset

---

## 1ï¸âƒ£ Import Required Libraries

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
```

ğŸ” Explanation:

numpy â†’ For numerical calculations.

pandas â†’ To load and manipulate tabular data.

matplotlib & seaborn â†’ For plotting graphs and visualizations.

train_test_split â†’ To split the dataset into training and testing sets.

StandardScaler â†’ Scales numeric features so all features contribute equally to distance (important for KNN).


2ï¸âƒ£ Download and Load Dataset
import kagglehub

# Download dataset
```
path = kagglehub.dataset_download("uciml/pima-indians-diabetes-database")
```

# Load CSV into DataFrame
```
df = pd.read_csv(path + "/diabetes.csv")
```

ğŸ” Explanation:

dataset_download() â†’ Downloads dataset from Kaggle.

```
pd.read_csv() â†’ Reads the CSV file into a pandas DataFrame.
```

df â†’ Contains the full dataset including features and target column.

3ï¸âƒ£ Explore the Dataset
a) Check Dataset Shape
```
df.shape
```

ğŸ” Explanation:

Returns the number of rows and columns.

Example output (768, 9) â†’ 768 samples and 9 columns (including target).

b) Check Data Types
```
df.dtypes
```

ğŸ” Explanation:

Confirms the datatype of each column (int64, float64).

Ensures all features are numeric for KNN.

c) View First 5 Rows
```
df.head()
```

ğŸ” Explanation:

Displays the first 5 records of the dataset.

Helps verify data is loaded correctly and understand feature values.

d) Check Class Distribution
```
df['Outcome'].value_counts()
```

ğŸ” Explanation:

Counts samples in each class:

0 â†’ No Diabetes

1 â†’ Diabetes

Example output:

0    500
1    268


Indicates dataset is imbalanced because class 0 has almost twice the samples of class 1.

e) Visualize Class Distribution
```
sns.countplot(x='Outcome', data=df)
plt.title("Class Distribution")
plt.show()
```

ğŸ” Explanation:

Creates a bar chart showing number of samples in each class.

Helps visually confirm class imbalance.


# ğŸ“¦ Step 3 â€“ Data Cleaning & Preprocessing


## 1ï¸âƒ£ Check Missing Values

```python
df.isnull().sum()
```

ğŸ” Explanation:

```
isnull().sum() checks for missing values in each column.
```

Missing values can cause errors in ML models.

In this dataset, there are no null values, but some medical features have zero values, which are unrealistic.

2ï¸âƒ£ Identify Zero Values in Medical Fields

```
zero_cols = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
(df[zero_cols] == 0).sum()
```

ğŸ” Explanation:

Some features like Glucose, BloodPressure, SkinThickness, Insulin, BMI should never be zero.

This code counts how many zero values exist in each of these columns.

3ï¸âƒ£ Replace Zero Values with Median

```
for col in zero_cols:
    df[col] = df[col].replace(0, df[col].median())
```

ğŸ” Explanation:

Replaces all zeros in the selected columns with the median of that column.

Median is used instead of mean because it is less affected by outliers.

This cleans the dataset so all features have realistic values.

4ï¸âƒ£ Check for Duplicates

```
df.duplicated().sum()
```

ğŸ” Explanation:

Checks if there are duplicate rows in the dataset.

Duplicate rows can bias the model.

5ï¸âƒ£ Remove Duplicates

```
df = df.drop_duplicates()
```

ğŸ” Explanation:

Removes duplicate rows from the dataset.

Ensures each record is unique for better model training.

6ï¸âƒ£ Define Features and Target

```
X = df.drop('Outcome', axis=1)

y = df['Outcome']
```

ğŸ” Explanation:

X â†’ All features (input variables)

y â†’ Target variable (Outcome: 0 or 1)

Prepares data for training/testing split.

7ï¸âƒ£ Train-Test Split

```
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42,
    stratify=y
)

```

ğŸ” Explanation:

Splits data into 80% training and 20% testing.

stratify=y â†’ Ensures same class ratio in train and test sets.

random_state=42 â†’ Ensures reproducibility.
